{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile,io\n",
    "import urllib.request\n",
    "#http framework to make Mapbox Matric API requests for routes\n",
    "import json # handle response as json\n",
    "import datetime # save timestamp\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import box\n",
    "from utils import *\n",
    "from closest_hospital import *\n",
    "from origin_destination import mapbox_matrix_API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres et création des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting WorldPop population data\n",
      "Beginning file download with urllib2...\n"
     ]
    }
   ],
   "source": [
    "MAPBOX_ACCESS_TOKEN = 'pk.eyJ1IjoibWlrbWVoMDEiLCJhIjoiY2s4YzE3ZnB3MGhoNDNsdHEzNG41N2VpNCJ9.mmNr9093SAZnZwULlKY2cQ'\n",
    "\n",
    "country = 'Tunisia'\n",
    "# Country code ISO3\n",
    "\n",
    "country_csv = pd.read_csv('wbccodes2014.csv')\n",
    "country_ISO = country_csv.set_index('country_name').loc[country,'country']\n",
    "output_path = os.path.join(country_ISO,'output')\n",
    "\n",
    "if not os.path.exists(country_ISO):\n",
    "    os.makedirs(country_ISO)\n",
    "    \n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "pop_folder = os.path.join(country_ISO,'Data Pop')\n",
    "location = 'WorldPop'\n",
    "print(\"Getting %s population data\"%(location) )\n",
    "\n",
    "if not os.path.exists(pop_folder):\n",
    "    os.makedirs(pop_folder)\n",
    "\n",
    "map_file = os.path.join(pop_folder, '%s_ppp_2019.tif'%country)\n",
    "\n",
    "# Downloading the raster file for population unless it's already downloaded\n",
    "if not os.path.isfile(map_file):\n",
    "    \n",
    "    print('Beginning file download with urllib2...')\n",
    "    url = 'ftp://ftp.worldpop.org.uk/GIS/Population/Global_2000_2020/2019/%s/%s_ppp_2019.tif' %(country_ISO,country_ISO)\n",
    "    urllib.request.urlretrieve(url, map_file)\n",
    "\n",
    "\n",
    "world_shp_path = 'World_shp'\n",
    "if not os.path.exists(world_shp_path):\n",
    "    os.makedirs(world_shp_path)\n",
    "    \n",
    "    # Downloading and extracting file\n",
    "    url_world_shp = 'https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip'\n",
    "    r = requests.get(url_world_shp, stream=True)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(world_shp_path)\n",
    "    \n",
    "world_shp = gpd.read_file(os.path.join(world_shp_path,'ne_50m_admin_0_countries.shp'))\n",
    "country_shp = world_shp[world_shp.ADM0_A3==country_ISO.upper()]\n",
    "\n",
    "country_centroid = country_shp.unary_union.centroid\n",
    "lat,lon = country_centroid.bounds[1],country_centroid.bounds[0]\n",
    "        \n",
    "# formula below based on :https://gis.stackexchange.com/a/190209/80697\n",
    "epsg = int(32700-np.round((45+lat)/90,0)*100+np.round((183+lon)/6,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the points from where we will seek for the closest hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4881\r"
     ]
    }
   ],
   "source": [
    "# Browsing the raster file, cutting it into squares of size #window\n",
    "# and computing bounds, population and number of null values\n",
    "\n",
    "origins = pd.DataFrame()\n",
    "window = 500\n",
    "with rasterio.open(map_file) as src:\n",
    "    for left_x in np.arange(0,src.width,window):\n",
    "        for top_y in np.arange(0,src.height,window):\n",
    "            out = get_pop(map_file,left_x,top_y,window,plot=False)\n",
    "            if out != {}:\n",
    "                origins = origins.append([out])\n",
    "        print(\"%i/%i\\r\"%(left_x,src.width),end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important : Run many times the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1564 regions need splitting\n",
      "done.2392\n",
      "We now have 16450 regions of min size 62, 13166 will be split in next round\n"
     ]
    }
   ],
   "source": [
    "# Splitting the original big squares into smaller squares.\n",
    "# You can run this cell as many times as you want depending \n",
    "# on the precisions you are seeking. Hence, running it multiple\n",
    "# times will ask for more computational power later.\n",
    "\n",
    "print(\"%i regions need splitting\"%len(origins[origins['split']==True]))\n",
    "olen = len(origins)\n",
    "for i in np.arange(olen):\n",
    "    print(\"%i/%i\\r\"%(i+1,olen),end=\"\")\n",
    "    if origins.iloc[i,origins.columns.get_loc('split')] == True:\n",
    "        origins.iloc[i,origins.columns.get_loc('split')] = 'done'\n",
    "        s = split(map_file,origins.iloc[i])\n",
    "        origins = origins.append(s,sort=False)\n",
    "print(\"done.\")\n",
    "print(\"We now have %i regions of min size %i, %i will be split in next round\"%\\\n",
    "      (len(origins),origins['window'].min(),len(origins[origins['split']==True])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 13166 regions of size 62, 13166 with population >0\n"
     ]
    }
   ],
   "source": [
    "origins = origins[origins['tot_pop']>0]\n",
    "origins = origins[origins['split']!='done']\n",
    "print(\"We have %i regions of size %i, %i with population >0\"%\n",
    "      (len(origins),min(origins['window']),len(origins[origins['tot_pop']>0])))\n",
    "\n",
    "#Transform it to geopandas frame where each row corresponds \n",
    "#to a location from where we will compute travel time to closest hospital\n",
    "origins = gpd.GeoDataFrame(origins,crs='epsg:4326', geometry=[Point(xy) for xy \\\n",
    "                                                              in zip(origins['center_lon'], origins['center_lat'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_path = os.path.join(country_ISO, 'Hospitals')\n",
    "if not os.path.exists(hospitals_path):\n",
    "    os.makedirs(hospitals_path)\n",
    "\n",
    "hospital_shp_path = os.path.join(hospitals_path, '%s.shp'%country)\n",
    "if not os.path.exists(hospital_shp_path):\n",
    "    # Downloading and extracting file\n",
    "    url_hospital = 'https://healthsites.io/data/shapefiles/%s.zip'%country\n",
    "    r = requests.get(url_hospital, stream=True)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(hospitals_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hospitals/clinics : 318\n"
     ]
    }
   ],
   "source": [
    "hospital_shp_path = os.path.join(hospitals_path, '%s.shp'%country)\n",
    "hospitals = gpd.read_file(hospital_shp_path)\n",
    "\n",
    "hospitals = hospitals[(hospitals.healthcare.isin(['clinic','hospital'])) | (hospitals.amenity.isin(['clinic','hospital'])) ]\n",
    "print('Number of hospitals/clinics :', len(hospitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = origins.reset_index(drop=True)\n",
    "origins_crs = convert_crs_gdf(origins,4326,epsg)\n",
    "hospitals_crs = hospitals.to_crs(epsg=epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = origins_crs.copy()\n",
    "o_type = 'hospital'\n",
    "cols=['t_'+o_type,'m_'+o_type,'so_'+o_type]\n",
    "for col in cols:\n",
    "    ori[col]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 13166\n",
      "Doing batch 1, from 0 to 12, of 13166\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehdi\\Desktop\\COVID\\utils.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: shapely.wkb.loads(geom))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing batch 1098, from 13164 to 13176, of 13166\n",
      "\n",
      "returning\n"
     ]
    }
   ],
   "source": [
    "output = mapbox_matrix_API(ori,hospitals_crs,MAPBOX_ACCESS_TOKEN,epsg, name='hospital',n_keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehdi\\Anaconda3\\envs\\GDAL\\lib\\site-packages\\geopandas\\tools\\sjoin.py:44: UserWarning: CRS of frames being joined does not match!\n",
      "  warn('CRS of frames being joined does not match!')\n",
      "C:\\Users\\mehdi\\Anaconda3\\envs\\GDAL\\lib\\site-packages\\geopandas\\io\\file.py:108: FionaDeprecationWarning: Use fiona.Env() instead.\n",
      "  with fiona.drivers():\n"
     ]
    }
   ],
   "source": [
    "origins['t_hospital'] = output['t_hospital']\n",
    "origins['so_hospital'] = output['so_hospital']\n",
    "\n",
    "# on garde que les points à l'intérieur du pays\n",
    "gpd.sjoin(origins,country_shp,op='within')[origins.columns].to_file(os.path.join(output_path, 'origins.shp'))\n",
    "hospitals.to_file(os.path.join(output_path, 'hospitals.shp'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GDAL]",
   "language": "python",
   "name": "conda-env-GDAL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
